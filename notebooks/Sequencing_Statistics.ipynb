{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Compute statistics for GISAID submission\n",
    "# ----------------------------------------\n",
    "#\n",
    "# Example usage:\n",
    "#   python run_gisaid-statistics.py -d data/WGS\n",
    "#\n",
    "# JHendry, 2021/01/01\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import getopt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from gisaid import *\n",
    "#import bamboolib as bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Compute GISAID statistics\n",
      "--------------------------------------------------------------------------------\n",
      "Command: /home/dan/.miniconda3/envs/nomads/lib/python3.7/site-packages/ipykernel_launcher.py -f /home/dan/.local/share/jupyter/runtime/kernel-9a0e2bde-671e-4fee-bea2-e34ee58930e1.json\n",
      "Run on host: PATH\n",
      "Operating system: Linux\n",
      "Machine: x86_64\n",
      "Started at: 2021-12-29 08:14:35\n",
      "================================================================================\n",
      "Preparing directories...\n",
      "  Rampart directory: /home/dan/TESTWGS/2_SampleList_and_Rampart\n",
      "  Artic directory: /home/dan/TESTWGS/3_Artic_Output\n",
      "  GISAID directory: /home/dan/TESTWGS/5_GISAID\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Compute GISAID statistics\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Command: %s\" % \" \".join(sys.argv))\n",
    "print(\"Run on host: %s\" % os.uname().nodename)\n",
    "print(\"Operating system: %s\" % os.uname().sysname)\n",
    "print(\"Machine: %s\" % os.uname().machine)\n",
    "print(\"Started at: %s\" % datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"=\" * 80)\n",
    "start_time = time.time()\n",
    "\n",
    "data_dir=\"/home/dan/TESTWGS\"\n",
    "\n",
    "# # PARSE CLI INPUT\n",
    "# print(\"Parsing command line inputs...\")\n",
    "# try:\n",
    "#     opts, args = getopt.getopt(sys.argv[1:], \":d:\")\n",
    "# except getopt.GetoptError:\n",
    "#     print(\"  Error parsing options.\")\n",
    "#     sys.exit(2)\n",
    "# for opt, value in opts:\n",
    "#     if opt == \"-d\":\n",
    "#         data_dir = value\n",
    "#         print(\"  Data directory: %s\" % data_dir)\n",
    "#     else:\n",
    "#         print(\"  Parameter %s not recognized.\" % opt)\n",
    "#         sys.exit(2)\n",
    "# print(\"Done.\")\n",
    "# print(\"\")\n",
    "\n",
    "\n",
    "# PREPARE DIRECTORIES\n",
    "print(\"Preparing directories...\")\n",
    "rampart_dir = os.path.join(data_dir, \"2_SampleList_and_Rampart\")\n",
    "artic_dir = os.path.join(data_dir, \"3_Artic_Output\")\n",
    "gisaid_dir = os.path.join(data_dir, \"5_GISAID\")\n",
    "if not os.path.isdir(gisaid_dir):\n",
    "    os.makedirs(gisaid_dir)\n",
    "print(\"  Rampart directory: %s\" % rampart_dir)\n",
    "print(\"  Artic directory: %s\" % artic_dir)\n",
    "print(\"  GISAID directory: %s\" % gisaid_dir)\n",
    "print(\"Done.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sample list details from Samples_Sequenced.csv...\n",
      "    1934 PCR reactions have been performed consisting of...\n",
      "        68 Controls\n",
      "        1866 Samples, of which....\n",
      "            1689 are unique\n",
      "Removing samples that have not been sequenced\n",
      "    1417 sequencing reactions have been performed consisting of...\n",
      "        61 Controls\n",
      "        1356 Samples, of which....\n",
      "            1259 are unique\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOAD SAMPLE METADATA\n",
    "print(\"Loading sample list details from Samples_Sequenced.csv...\")\n",
    "samples_df = pd.read_csv(os.path.join(rampart_dir, \"Samples_Sequenced.csv\"))\n",
    "print(\"    %s PCR reactions have been performed consisting of...\" % samples_df.shape[0])\n",
    "print(\"        %s Controls\" % samples_df[samples_df.Type=='Control'].shape[0])\n",
    "print(\"        %s Samples, of which....\" % samples_df[samples_df.Type=='Sample'].shape[0])\n",
    "print(\"            %s are unique\" % samples_df[samples_df.Type=='Sample'][\"SampleID\"].unique().shape[0])\n",
    "\n",
    "print(\"Removing samples that have not been sequenced\")\n",
    "seqsamples_df = samples_df[samples_df.SeqRun.notnull()]\n",
    "print(\"    %s sequencing reactions have been performed consisting of...\" % seqsamples_df.shape[0])\n",
    "print(\"        %s Controls\" % seqsamples_df[seqsamples_df.Type=='Control'].shape[0])\n",
    "print(\"        %s Samples, of which....\" % seqsamples_df[seqsamples_df.Type=='Sample'].shape[0])\n",
    "print(\"            %s are unique\" % seqsamples_df[seqsamples_df.Type=='Sample'][\"SampleID\"].unique().shape[0])\n",
    "# Map from Unique ID to barcode ID\n",
    "sample_dt = { row[\"SeqID\"]: row[\"SeqBarcode\"] for _, row in seqsamples_df.iterrows() }\n",
    "print(\"Done.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# RUN ON NEON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# EXAMINE CONTENTS\n",
    "print(\"Examining directory contents...\")\n",
    "contents_dt = {}\n",
    "rs = os.listdir(artic_dir)\n",
    "for r in os.listdir(artic_dir):\n",
    "    if os.path.isdir(os.path.join(artic_dir, r)) and r.startswith(\"C\"):\n",
    "        d = os.path.join(artic_dir, r, \"processed\")\n",
    "        n_samples = sum([1 for s in os.listdir(d) \n",
    "                         if os.path.isdir(os.path.join(d, s))])\n",
    "        contents_dt[r] = n_samples\n",
    "print(\"  Run\\tNo. samples\")\n",
    "for d, n in contents_dt.items():\n",
    "    print(\"  %s\\t%d\" % (d, n))\n",
    "print(\"  Total runs: %d\" % len(contents_dt.keys()))\n",
    "print(\"  Total samples: %d\" % sum(contents_dt.values()))\n",
    "print(\"Done.\")\n",
    "print(\"\")\n",
    "\n",
    "# Prepare storage\n",
    "dts = [] #data per sample\n",
    "dtr = [] #data per run\n",
    "\n",
    "# LOAD SAMPLE METADATA\n",
    "print(\"Loading sample list details from Samples_Sequenced.csv...\")\n",
    "samples_df = pd.read_csv(os.path.join(rampart_dir, \"Samples_Sequenced.csv\"))\n",
    "print(\"    %s PCR reactions have been performed consisting of...\" % samples_df.shape[0])\n",
    "print(\"        %s Controls\" % samples_df[samples_df.Type=='Control'].shape[0])\n",
    "print(\"        %s Samples, of which....\" % samples_df[samples_df.Type=='Sample'].shape[0])\n",
    "print(\"            %s are unique\" % samples_df[samples_df.Type=='Sample'][\"SampleID\"].unique().shape[0])\n",
    "\n",
    "print(\"Removing samples that have not been sequenced\")\n",
    "seqsamples_df = samples_df[samples_df.SeqRun.notnull()]\n",
    "print(\"    %s sequencing reactions have been performed consisting of...\" % seqsamples_df.shape[0])\n",
    "print(\"        %s Controls\" % seqsamples_df[seqsamples_df.Type=='Control'].shape[0])\n",
    "print(\"        %s Samples, of which....\" % seqsamples_df[seqsamples_df.Type=='Sample'].shape[0])\n",
    "print(\"            %s are unique\" % seqsamples_df[seqsamples_df.Type=='Sample'][\"SampleID\"].unique().shape[0])\n",
    "# Map from Unique ID to barcode ID\n",
    "sample_dt = { row[\"SeqID\"]: row[\"SeqBarcode\"] for _, row in seqsamples_df.iterrows() }\n",
    "print(\"Done.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# COMPUTE SEQUENCING STATISTICS\n",
    "print(\"Computing sequencing statistics...\")\n",
    "\n",
    "# Iterate over runs\n",
    "print(\"  Run  Samples/Complete\")\n",
    "for r in rs:\n",
    "    \n",
    "    # Define run directory\n",
    "    run_dir = os.path.join(artic_dir, r)\n",
    "    if os.path.isdir(run_dir) and r.startswith(\"C\"):\n",
    "        d = os.path.join(run_dir, \"processed\")\n",
    "        \n",
    "        #Dictionary for a count of the number of reads that are unclassified\n",
    "        stats_run = {}\n",
    "        stats_run[\"SeqRun\"] = r\n",
    "        \n",
    "        #Need to count the number of unclassified reads just once per run\n",
    "        unclass_dir = os.path.join(d.replace(\"processed\", \"fastq/unclassified\"))\n",
    "        \n",
    "        try:\n",
    "            t = 0 #count number of reads\n",
    "            for h, u in enumerate(os.listdir(unclass_dir)): #for each fastq\n",
    "                current = calc_fastq_total_reads(os.path.join(unclass_dir, u))\n",
    "                t = t+current\n",
    "            stats_run.update({\"total_reads\": t})\n",
    "        except:\n",
    "            stats_run.update({\"total_reads\": 0})\n",
    "        \n",
    "        # Store unclass results\n",
    "        dtr.append(stats_run)\n",
    "        \n",
    "        #Iterate over samples\n",
    "        n_total = len(os.listdir(d))\n",
    "        for i, s in enumerate(os.listdir(d)):\n",
    "            \n",
    "            # Print progress\n",
    "            sys.stdout.write(\"\\r\")\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"  %s %d/%d\" % (r, i+1, n_total))\n",
    "            \n",
    "            # Define sample directory\n",
    "            sample_dir = os.path.join(d, s)\n",
    "            \n",
    "            # Define sample barcode\n",
    "            b = sample_dt[s]\n",
    "            \n",
    "            # Identifiers\n",
    "            stats_dt = {}\n",
    "            stats_dt[\"SeqRun\"] = r\n",
    "            stats_dt[\"SeqID\"] = s\n",
    "            stats_dt[\"SeqBarcode\"] = b\n",
    "            \n",
    "            # Calc. most statistics from coverage file\n",
    "            coverage_df = load_coverage_files(sample_dir)\n",
    "            stats_dt.update(calc_gisaid_stats(coverage_df))\n",
    "            \n",
    "            # Calc. Ns per 100kbp from consensus FASTA\n",
    "            consensus_path = os.path.join(sample_dir, \"%s.consensus.fasta\" % s)\n",
    "            ns_per_100kbp = calc_ns_per_100kbp(consensus_path, verbose=False)\n",
    "            stats_dt.update({\"ns_per_100kbp\": ns_per_100kbp})\n",
    "            \n",
    "            # Identify fastq files for analysis\n",
    "            b_fn = os.path.join(d.replace(\"processed\", \"fastq\"), \"C%02d_barcode%02d.fastq\" % (int(r[1:]), b))\n",
    "            print(b_fn)\n",
    "            try:\n",
    "                # Calc. sequencing depth from .fastq\n",
    "                sequencing_depth_avg_fastq = calc_avg_seq_depth(b_fn, genome_length=stats_dt[\"ref_genome_length\"])\n",
    "                stats_dt.update({\"sequencing_depth_avg_fastq\": sequencing_depth_avg_fastq})\n",
    "                # Calc. number of reads per barcode\n",
    "                total_reads = calc_fastq_total_reads(b_fn)\n",
    "                stats_dt.update({\"total_reads\": total_reads})\n",
    "            except:\n",
    "                stats_dt.update({\"sequencing_depth_avg_fastq\": 0})\n",
    "            \n",
    "            # Store\n",
    "            dts.append(stats_dt)\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        \n",
    "# Create data frame\n",
    "stats_df = pd.DataFrame(dts)\n",
    "print(\"Done.\")\n",
    "\n",
    "#Calculate stats from the fasta file if present\n",
    "fa_path = os.path.join(gisaid_dir, \"allsequences.fasta\")\n",
    "if os.path.isfile(fa_path):    \n",
    "    print(\"Calculating statistics from the multi-fasta file for all sequences\")\n",
    "    #Generate statistics from the multi-fasta file\n",
    "    fasta_dt = fasta_stats(fa_path)\n",
    "    #Turn dictionary into a dataframe in correct orientation\n",
    "    fasta_df = pd.DataFrame.from_dict(fasta_dt, orient ='index')\n",
    "    #Reset the index so it is a column you can match on\n",
    "    fasta_df.reset_index(inplace=True)\n",
    "    print(\"   Stats generated for %d sequences\" % len(fasta_df))\n",
    "\n",
    "    #Merge with the other gisaid stats\n",
    "    print(\"Merging fasta stats with bam / fastq stats (stats_df)\")\n",
    "    print(\"   %d records in stats_df\" % len(stats_df))\n",
    "    stats_df = pd.merge(stats_df, fasta_df, how='outer', left_on='SeqID', right_on='index')\n",
    "    print(\"   %d records remaining\" % len(stats_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    All sequence IDs are unique\n",
      "\n",
      "Statistics for a total of 1248 samples have been calculated\n",
      "    Data output to intermediates/stats.csv\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check that all the stats sequence IDs are unique\n",
    "if len(stats_df[\"SeqID\"]) == len(set(stats_df[\"SeqID\"])):\n",
    "    print(\"    All sequence IDs are unique\")\n",
    "else:\n",
    "    print(\"    Duplicate sequence IDs identified\")\n",
    "    l_func = lambda x, y: list((set(x)- set(y))) + list((set(y)- set(x))) \n",
    "    non_match = l_func(stats_df[\"SeqID\"], stats_df[\"SeqID\"].unique()) \n",
    "    print(\"    Non-match elements: \", non_match)\n",
    "    print(\"Exiting script\")\n",
    "    exit()\n",
    "print(\"\")\n",
    "print(\"Statistics for a total of %s samples have been calculated\" % stats_df.shape[0])\n",
    "stats_fn = \"intermediates/stats.csv\"\n",
    "stats_df.to_csv(os.path.join(gisaid_dir, stats_fn), index=False)\n",
    "print(\"    Data output to %s\" % stats_fn)\n",
    "print(\"Done.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch back to Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#START HERE IF YOU HAVE ALREADY GENERATED THE stats_df.csv file\n",
    "stats_df = pd.read_csv(os.path.join(gisaid_dir, \"intermediates/stats.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Combining with other data outputs\n",
      "  PANGO and nextclade files found. Reading in data\n"
     ]
    }
   ],
   "source": [
    "#Find the other stats files for merging\n",
    "print(\"-\" * 80)\n",
    "print(\"Combining with other data outputs\")\n",
    "if os.path.isfile(os.path.join(gisaid_dir,\"intermediates/lineage_report.csv\")) and os.path.isfile(os.path.join(gisaid_dir,\"intermediates/nextclade.csv\")):\n",
    "    print (\"  PANGO and nextclade files found. Reading in data\")\n",
    "    pango_df = pd.read_csv(os.path.join(gisaid_dir, \"intermediates/lineage_report.csv\"))\n",
    "    nextclade_df = pd.read_csv(os.path.join(gisaid_dir, \"intermediates/nextclade.csv\"), sep=';')\n",
    "elif os.path.isfile(os.path.join(gisaid_dir,\"intermediates/lineage_report.csv\")):\n",
    "    print (\"  Only PANGO file found. Exiting script\")\n",
    "    #exit()\n",
    "elif os.path.isfile(os.path.join(gisaid_dir,\"intermediates/nextclade.csv\")):\n",
    "    print (\"  Only nextclade file found. Exiting script\")\n",
    "    #exit()\n",
    "else:\n",
    "    print (\"  PANGO and nextclade files not found. Exiting script\")\n",
    "    #exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  All sequences match across the three files\n",
      "  Merging gisaid (n=1379), pango (n=1379) and nextclade (n=1379) data together\n",
      "  Total remaining records = 1379\n",
      "  Removing unwanted column headings from final summary file of samples sequenced\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check that all sequences are represented. If not this suggests that some of the fasta concatenation has failed or that the wrong sequence name has been used\n",
    "l_func = lambda x, y: list((set(x)- set(y))) + list((set(y)- set(x))) \n",
    "non_match_gp = l_func(stats_df[\"SeqID\"], pango_df[\"taxon\"]) \n",
    "non_match_gn = l_func(stats_df[\"SeqID\"], nextclade_df[\"seqName\"]) \n",
    "if len(non_match_gp) > 0 :\n",
    "    print(\"  A total of %s sequence ids do not match between gisaid and PANGO\" % len(non_match_gp))\n",
    "    #exit()\n",
    "elif len(non_match_gn) > 0 :\n",
    "    print(\"  A total of %s sequence ids do not match between gisaid and nextclade\" % len(non_match_gn))\n",
    "    #exit()\n",
    "else:\n",
    "    print(\"  All sequences match across the three files\")\n",
    "\n",
    "print(\"  Merging gisaid (n=%d), pango (n=%d) and nextclade (n=%d) data together\" % (stats_df.shape[0], pango_df.shape[0], nextclade_df.shape[0]))\n",
    "Merge1 = pd.merge(stats_df, pango_df, how='outer', left_on='SeqID', right_on='taxon')\n",
    "Merge2 = pd.merge(Merge1, nextclade_df, how='outer', left_on='SeqID', right_on='seqName')\n",
    "print(\"  Total remaining records = %d\" % (Merge2.shape[0]))\n",
    "print(\"  Removing unwanted column headings from final summary file of samples sequenced\")\n",
    "gisaidcols = ['ref_genome_length']\n",
    "pangocols = (['taxon'])\n",
    "nextcladecols = ['seqName']\n",
    "dropcols = gisaidcols + pangocols + nextcladecols\n",
    "sequenced_df = Merge2.drop(dropcols, axis=1)\n",
    "print(\"Done.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Merging sequenced samples with sample list...\n",
      "  No. samples...\n",
      "    ...in sample list: 1417\n",
      "    ...with consensus sequence: 1379\n",
      "    ...after merging: 1417\n",
      "Done.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Identifying submittable samples\n",
      "   1417 entries identified\n",
      "   1356 marked as samples\n",
      "   1307 samples not marked to exclude\n",
      "   1233 samples with seq depth > 50x\n",
      "   1217 samples with coverage breadth > 50%\n",
      "Samples remaining: 1217\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MERGE WITH SAMPLE LIST\n",
    "print(\"-\" * 80)\n",
    "print(\"Merging sequenced samples with sample list...\")\n",
    "print(\"  No. samples...\")\n",
    "print(\"    ...in sample list: %d\" % seqsamples_df.shape[0])\n",
    "print(\"    ...with consensus sequence: %d\" % stats_df.shape[0])\n",
    "merged_df = pd.merge(left=seqsamples_df,\n",
    "                     right=sequenced_df,\n",
    "                     left_on=[\"SeqRun\", \"SeqBarcode\", \"SeqID\"],\n",
    "                     right_on=[\"SeqRun\", \"SeqBarcode\", \"SeqID\"],\n",
    "                    how='outer')\n",
    "print(\"    ...after merging: %d\" % merged_df.shape[0])\n",
    "print(\"Done.\")\n",
    "print(\"\")\n",
    "\n",
    "qc_depth = 50\n",
    "qc_breadth = 50\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Identifying submittable samples\")\n",
    "print(\"   %d entries identified\" % merged_df.shape[0])\n",
    "keepers_df = merged_df.query(\"Type == 'Sample'\", inplace=False)\n",
    "print(\"   %d marked as samples\" % keepers_df.shape[0])\n",
    "keepers_df = keepers_df.query(\"ExcludeSample != 'Y'\", inplace=False)\n",
    "print(\"   %d samples not marked to exclude\" % keepers_df.shape[0])\n",
    "keepers_df = keepers_df.query(\"sequencing_depth_avg >= @qc_depth\", inplace=False)\n",
    "print(\"   %d samples with seq depth > %dx\" % (keepers_df.shape[0], qc_depth))\n",
    "keepers_df = keepers_df.query(\"coverage_breadth >= @qc_breadth\", inplace=False)\n",
    "print(\"   %d samples with coverage breadth > %d%%\" % (keepers_df.shape[0], qc_breadth))\n",
    "print(\"Samples remaining: %d\" % keepers_df.shape[0])\n",
    "print(\"Done\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying highest depth consensus where samples have been sequenced multiple times:\n",
      "\n",
      "  Sample    No. dup.  SeqID            Breadth  Depth  Note        Top_SeqID  Top_Breadth  Top_Depth\n",
      "  111       9         111              99       451 \n",
      "  112849    2         112849           99       446 \n",
      "  112851    2         112851           99       309 \n",
      "  113113    2         113113           99       445 \n",
      "  113115    2         113115           99       451 \n",
      "  125       2         125              99       437 \n",
      "  128376    2         128376           99       442 \n",
      "  128421    2         128421           99       442 \n",
      "  230       5         230              92       153 \n",
      "  24445     2         24445            99       449      WARNING  24445a           99       451 \n",
      "  29        3         29a              99       440 \n",
      "  34698     2         34698a           99       451 \n",
      "  37084     2         37084            99       439 \n",
      "  39680     2         39680_b          94       363 \n",
      "  39683     3         39683_b          97       439      WARNING  39683            99       442 \n",
      "  39685     2         39685_a          97       427 \n",
      "  39689     2         39689_b          97       439 \n",
      "  39754     2         39754            97       439      WARNING  39754_b          99       446 \n",
      "  39755     2         39755_a          97       434 \n",
      "  40401     2         40401            97       428      WARNING  40401_b          99       443 \n",
      "  40805     2         40805            97       380      WARNING  40805_b          97       394 \n",
      "  41147     2         41147            97       416      WARNING  41147_b          99       444 \n",
      "  67        4         67a              99       438 \n",
      "  79718     2         79718_C19b       99       445 \n",
      "  84488     2         84488            99       419 \n",
      "  85295     2         85295            99       449 \n",
      "  85476     2         85476            99       408 \n",
      "  97819     2         97819_P33        99       447 \n",
      "  97824     2         97824            99       414 \n",
      "  97825     2         97825            99       431 \n",
      "  97881     2         97881            99       451 \n",
      "  97930     2         97930            99       439 \n",
      "  97932     2         97932            99       440 \n",
      "  97968     2         97968            99       440 \n",
      "  98106     2         98106_P33        99       447 \n",
      "  98119     2         98119_P33        99       445 \n",
      "  98123     2         98123            99       442 \n",
      "  98125     2         98125            99       434 \n",
      "  98141     2         98141_P33        99       438 \n",
      "  98144     2         98144_P33        99       451 \n",
      "  98145     2         98145_P33        99       451 \n",
      "  98165     2         98165_P33        99       451 \n",
      "  98170     2         98170_P33        99       451 \n",
      "  98177     2         98177_P33        99       452 \n",
      "  98193     2         98193_P33        99       451 \n",
      "  98231     2         98231_P33        99       444 \n",
      "  98254     2         98254            99       437 \n",
      "  98257     2         98257_P33        99       450 \n",
      "  98281     2         98281            99       448 \n",
      "  98298     2         98298_P33        99       434 \n",
      "  98305     2         98305_P33        99       444 \n",
      "  98550     2         98550            99       405 \n",
      "\n",
      "  Total Submittable samples: 1234\n",
      "Done.\n",
      "\n",
      "Marking submittable samples in df\n"
     ]
    }
   ],
   "source": [
    "#Highlight duplicates\n",
    "print(\"Identifying highest depth consensus where samples have been sequenced multiple times:\")\n",
    "print(\"\")\n",
    "\n",
    "l_dfs = []\n",
    "print(\"  {:<8}  {:<8}  {:<15}  {:<4}  {:<4}  {:<10}  {:<4}  {:<4}  {:<4}\"\n",
    "      .format(\"Sample\", \"No. dup.\", \"SeqID\", \"Breadth\", \"Depth\", \"Note\", \"Top_SeqID\",\"Top_Breadth\", \"Top_Depth\",))\n",
    "\n",
    "for n, sdf in keepers_df.groupby(\"SampleID\"):\n",
    "    n_dup = sdf.shape[0]\n",
    "    if n_dup > 1:\n",
    "        keep = sdf.sort_values(by =['GISAID_Accession_Number','coverage_breadth','sequencing_depth_avg'], \n",
    "                               ascending=[False,False,False], na_position='last').iloc[0]\n",
    "        #Check that there are not multiple GISAID entries or that a better seq exists for submission\n",
    "        if sdf[sdf['GISAID_Accession_Number'].notna()].shape[0] > 0 :\n",
    "            #Identify best SeqID scoring entry\n",
    "            top = sdf.sort_values(by =['coverage_breadth','sequencing_depth_avg'], \n",
    "                                       ascending=[False,False], na_position='last').iloc[0]\n",
    "            if top['SeqID'] != keep['SeqID'] :\n",
    "                print(\"  {:<8}  {:<8}  {:<15}  {:<4.0f}     {:<4.0f}     {:<4}  {:<15}  {:<4.0f}     {:<4.0f}\".format(\n",
    "                    n, n_dup, keep[\"SeqID\"], keep[\"coverage_breadth\"], keep[\"sequencing_depth_avg\"], \n",
    "                    \"WARNING\", top['SeqID'], top['coverage_breadth'], top['sequencing_depth_avg']))\n",
    "            else:\n",
    "                print(\"  {:<8}  {:<8}  {:<15}  {:<4.0f}     {:<4.0f}\".format(\n",
    "                    n, n_dup, keep[\"SeqID\"], keep[\"coverage_breadth\"], keep[\"sequencing_depth_avg\"]))\n",
    "    else:\n",
    "        keep = sdf.iloc[0]\n",
    "    l_dfs.append(keep)\n",
    "\n",
    "keepers_df = pd.concat(l_dfs, 1).transpose()\n",
    "print(\"\")\n",
    "print(\"  Total Submittable samples: %d\" % keepers_df.shape[0])\n",
    "print(\"Done.\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Marking submittable samples in df\")\n",
    "keeperslist_df = keepers_df.filter(['SeqID','SeqRun','SeqBarcode'])\n",
    "keeperslist_df[\"Submittable\"] = True\n",
    "alldata_df = pd.merge(left=merged_df,\n",
    "                     right=keeperslist_df,\n",
    "                     left_on=[\"SeqRun\", \"SeqBarcode\", \"SeqID\"],\n",
    "                     right_on=[\"SeqRun\", \"SeqBarcode\", \"SeqID\"],\n",
    "                    how='outer')\n",
    "alldata_df = alldata_df.fillna({'Submittable' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO DO\n",
    "\n",
    "# Wanted to do a check to ensure that dates are correctly formatted\n",
    "#metadata_df['TEST'] = metadata_df.apply(lambda x: datetime.date(x[int('Year')], x[int('Month')], x[int('Day')]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Incorporating metadata...\n",
      "  Metadata identified for : 1272 samples\n",
      "  Sequencing data for : 1234 samples\n",
      "  Merging metadata with sequence data...\n",
      "Done\n",
      "  Total samples retained: 1234\n",
      "ERROR: 8 records have a SeqDate before the SpecimenDate\n",
      "      SeqID SeqRun SpecimenDate    SeqDate\n",
      "793  107627    C31   2021-06-30 2021-06-29\n",
      "794  107628    C31   2021-06-30 2021-06-29\n",
      "795  107630    C31   2021-06-30 2021-06-29\n",
      "796  107633    C31   2021-06-30 2021-06-29\n",
      "797  107635    C31   2021-06-30 2021-06-29\n",
      "798  107636    C31   2021-06-30 2021-06-29\n",
      "799  107638    C31   2021-06-30 2021-06-29\n",
      "800  107640    C31   2021-06-30 2021-06-29\n",
      "\n",
      "  Writing out all sequence data with metadata...\n",
      "  To: /home/dan/TESTWGS/5_GISAID/Samples_Sequenced_With_Metadata.csv\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Merging in metadata\n",
    "print(\"-\" * 80)\n",
    "print(\"Incorporating metadata...\")\n",
    "\n",
    "metadata_fn=\"Metadata.csv\"\n",
    "metadata_df = pd.read_csv(os.path.join(rampart_dir, metadata_fn))\n",
    "\n",
    "#Add in column highlighting any missing data\n",
    "check_cols = [\"Province\", \"District\", \"SpecimenDate\"] #List of key columns\n",
    "missing_data = [] #To house the new column of data\n",
    "for _, row in metadata_df.iterrows(): #Iterate over rows\n",
    "    m = [] #List to hold output from an individual column\n",
    "    for col in check_cols: #Iterate over columns we want to check\n",
    "        if row[col] != row[col]: #Only NaN is not equal to itself\n",
    "            m.append(col) #add data to list\n",
    "    missing_data.append(\", \".join(m)) #Join as a new string\n",
    "metadata_df[\"MissingMetadata\"] = missing_data\n",
    "\n",
    "print(\"  Metadata identified for : %d samples\" % metadata_df.shape[0])\n",
    "print(\"  Sequencing data for : %d samples\" % keepers_df.shape[0])\n",
    "print(\"  Merging metadata with sequence data...\")\n",
    "samplemeta_df = pd.merge(left=metadata_df,\n",
    "                     right=keepers_df,\n",
    "                     left_on=[\"SampleID\"],\n",
    "                     right_on=[\"SampleID\"],\n",
    "                    how='inner')\n",
    "print(\"Done\")\n",
    "print(\"  Total samples retained: %d\" % keepers_df.shape[0])\n",
    "\n",
    "#Change date fields from str to date\n",
    "samplemeta_df['SeqDate'] = pd.to_datetime(samplemeta_df['SeqDate'], format='%d/%m/%Y')\n",
    "\n",
    "#Drop date field as excel keeps changing format and rebuild from separate cols\n",
    "samplemeta_df.drop(labels='SpecimenDate', axis =1)\n",
    "samplemeta_df['SpecimenDate'] = pd.to_datetime(samplemeta_df['SpecimenDate'], format='%d/%m/%Y')\n",
    "#Sanity check on sample date and sequencing date\n",
    "samplemeta_df['DateError'] = samplemeta_df['SeqDate'] < samplemeta_df['SpecimenDate']\n",
    "\n",
    "if samplemeta_df[samplemeta_df.DateError==True].shape[0] > 0 :\n",
    "    print(\"ERROR: %d records have a SeqDate before the SpecimenDate\" % samplemeta_df[samplemeta_df.DateError==True].shape[0])\n",
    "    print(samplemeta_df[samplemeta_df.DateError==True][['SeqID','SeqRun','SpecimenDate','SeqDate']])\n",
    "else:\n",
    "    print(\"All records have a SeqDate after the SpecimenDate\")\n",
    "    samplemeta_df.drop(columns = ['DateError'], inplace = True)\n",
    "print(\"\")\n",
    "\n",
    "#WRITE RESULTS\n",
    "print(\"  Writing out all sequence data with metadata...\")\n",
    "output_fn = \"Samples_Sequenced_With_Metadata.csv\"\n",
    "samplemeta_df.to_csv(os.path.join(gisaid_dir, output_fn), sep = ',', index=False)\n",
    "print(\"  To: %s\" % os.path.join(gisaid_dir, output_fn))\n",
    "print(\"Done.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column to alldata_df that highlights if a sample has metadata or not\n",
    "SID_meta = list(samplemeta_df['SampleID'])\n",
    "meta = []\n",
    "for s in alldata_df['SampleID'] :\n",
    "    if s in SID_meta :\n",
    "        r = True\n",
    "    else :\n",
    "        r = False\n",
    "    meta.append(r)\n",
    "    \n",
    "alldata_df['Metadata available'] = meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing all sequencing data...\n",
      "    To: /home/dan/TESTWGS/5_GISAID/allsequencedata.csv\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WRITE RESULTS\n",
    "print(\"Writing all sequencing data...\")\n",
    "output_fn = \"allsequencedata.csv\"\n",
    "alldata_df.to_csv(os.path.join(gisaid_dir, output_fn), sep = ',', index=False)\n",
    "print(\"    To: %s\" % os.path.join(gisaid_dir, output_fn))\n",
    "print(\"Done.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on NEON or provide the dtr datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#START HERE IF YOU HAVE ALREADY GENERATED THE dtrdf.csv file\n",
    "dtr = pd.read_csv(os.path.join(rampart_dir, \"dtr.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Summarising output per run...\n",
      "  For all samples added to a sequencing run\n",
      "  For all samples with a consensus\n",
      "  For all samples with depth >50X and breadth >50% \n",
      "  Done\n",
      "  Writing results...\n",
      "  To: /media/dan/9a4218fd-182c-4c9e-81a2-cfa141d4ae0e/WGS/5_GISAID/runsummary.csv\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate per run summaries\n",
    "print(\"-\" * 80)\n",
    "print(\"Summarising output per run...\")\n",
    "print(\"  For all samples added to a sequencing run\")\n",
    "sample_sum = seqsamples_df.groupby(\"SeqRun\").agg(\n",
    "    samples=('SeqID', 'count')\n",
    ")\n",
    "\n",
    "print(\"  For all samples with a consensus\")\n",
    "sequenced_sum = sequenced_df.groupby('SeqRun').agg(\n",
    "    all_consensus=('SeqID','count'),\n",
    "    all_depth_min=('assembly_coverage_depth',min),\n",
    "    all_depth_max=('assembly_coverage_depth',max),\n",
    "    all_score_mean=('qc.overallScore','mean')\n",
    ")\n",
    "\n",
    "print(\"  For all samples with depth >{:<2}X and breadth >{:<2}% \".format(qc_depth, qc_breadth))\n",
    "qc = sequenced_df[(sequenced_df.sequencing_depth_avg > qc_depth) & \n",
    "                    (sequenced_df.coverage_breadth > qc_breadth)]\n",
    "qc_sum = qc.groupby('SeqRun').agg(\n",
    "    qc_consensus=('SeqID','count'),\n",
    "    qc_depth_min=('assembly_coverage_depth',min),\n",
    "    qc_depth_max=('assembly_coverage_depth',max),\n",
    "    qc_score_mean=('qc.overallScore','mean')\n",
    ")\n",
    "\n",
    "#Merge datasets together\n",
    "sum1_df = pd.merge(left=sample_sum,\n",
    "                     right=sequenced_sum,\n",
    "                     left_on=[\"SeqRun\"],\n",
    "                     right_on=[\"SeqRun\"],\n",
    "                     how=\"outer\")\n",
    "sum1_df['all_success'] = sum1_df['all_consensus']/sum1_df['samples']\n",
    "\n",
    "runsummary_df = pd.merge(left=sum1_df,\n",
    "                     right=qc_sum,\n",
    "                     left_on=[\"SeqRun\"],\n",
    "                     right_on=[\"SeqRun\"],\n",
    "                     how=\"outer\")\n",
    "runsummary_df['qc_success'] = runsummary_df['qc_consensus']/runsummary_df['samples']\n",
    "\n",
    "#Create dataframes for barcoded and unclassified reads per run\n",
    "bc = stats_df[['SeqRun','total_reads']].groupby('SeqRun').sum()\n",
    "bc.rename(columns = {'total_reads':'barcoded_reads'}, inplace = True)\n",
    "un = pd.DataFrame(dtr).groupby('SeqRun').sum()\n",
    "un.rename(columns = {'total_reads':'unclassified_reads'}, inplace = True)\n",
    "#Merge into runsummary_df\n",
    "runsummary_df = pd.merge(left=runsummary_df,\n",
    "                     right=bc,\n",
    "                     left_on=[\"SeqRun\"],\n",
    "                     right_on=[\"SeqRun\"],\n",
    "                     how=\"outer\")\n",
    "runsummary_df = pd.merge(left=runsummary_df,\n",
    "                     right=un,\n",
    "                     left_on=[\"SeqRun\"],\n",
    "                     right_on=[\"SeqRun\"],\n",
    "                     how=\"outer\")\n",
    "#Calculate barcoding efficiency\n",
    "runsummary_df[\"barcoding_efficiency\"] = runsummary_df['barcoded_reads'] / (runsummary_df['unclassified_reads'] + runsummary_df['barcoded_reads'])\n",
    "\n",
    "print(\"  Done\")\n",
    "\n",
    "# WRITE RESULTS\n",
    "print(\"  Writing results...\")\n",
    "runsummary_fn = \"runsummary.csv\"\n",
    "runsummary_df.to_csv(os.path.join(gisaid_dir, runsummary_fn), index=True)\n",
    "print(\"  To: %s\" % os.path.join(gisaid_dir, runsummary_fn))\n",
    "print(\"Done.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Runtime: 0:04:06.323298\n",
      "Finished at: 2021-05-19 10:08:05\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 80)\n",
    "print(\"Runtime: %s\" % str(datetime.timedelta(seconds=time.time() - start_time)))\n",
    "print(\"Finished at: %s\" % datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
